\section{Preliminaries}
\label{sec:preliminaries}


Here is an overview of concepts you are supposed to have seen in earlier courses.
We will use these concepts over and over in the rest of the course.

\subsection*{Theory and Exercises}

\Opensolutionfile{hint}
\Opensolutionfile{ans}


\recall{Small $o$ notation}.   The function $f(h)=o(h)$ means that $f$ is such $f(h)/h \to 0$ as $h\to 0$.  If we write $f(h) = o(h)$ it is implicit that $|h| \ll 1$. 
\begin{exercise}[\faCalculator]
  Let $c$ be a constant (in $\R$) and the functions $f$ and $g$ both of $o(h)$. Then show that (1) $f(h) \to 0$ when $h\to 0$, (2) $c\cdot f = o(h)$, (3) $f+g=o(h)$, and (4) $f\cdot g=o(h)$. 
 \begin{solution}
In fact (1) is trivial: $|f(h)| \leq |f(h)/h|$ when $|h| < 1$. But it is given that the right-hand side goes to zero.  For (2) and (3):
\begin{align*}
\lim_{h\to 0} \frac{c f(h)}{h} &=  c \lim_{h\to 0} \frac{f(h)}{h} = 0, \; \text{as } f = o(h), \\
\lim_{h\to 0} \frac{f(h) + g(h)} h &= \lim_{h\to 0} \frac{f(h)} h + \lim_{h\to 0} \frac{g(h)} h = 0.
\end{align*}
For (4), use  the Algebraic Limit Theorem,
\begin{align*}
\lim_{h\to 0} \frac{f(h)g(h)}{h} &= \lim_{h\to 0} h \frac{f(h)}{h} \frac{g(h)}{h} \\
&= \lim_{h\to 0} h \lim_{h\to 0} \frac{f(h)}{h} \lim_{h\to 0} \frac{g(h)}{h} \\
&= 0 \cdot 0 \cdot 0 = 0.
\end{align*}
  \end{solution}
\end{exercise}

You should know that
\begin{subequations}
  \begin{align}
    (a+b)^n &= \sum_{i=0}^n {n \choose i} a^{n-i} b^i, \label{eq:71}\\
e^x &= \lim_{n\to\infty} (1+x/n)^n, \label{eq:65}\\
  e^x &= 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots = \sum_{k=0}^{\infty} \frac{x^k}{k!}, \label{eq:76}\\
    \1{A} &=
            \begin{cases}
              1, & \text{ if $A$ is true}, \\
              0, & \text{ if $A$ is false}.
            \end{cases}, \quad\text{Indicator variable}.
\end{align}
\end{subequations}

\begin{exercise}[\faFlask]
  Why is $e^{x} = 1 +x + o(x)$?
  \begin{solution}
    When $|x|\ll 1$, the  terms with $n\geq 2$ in~\eqref{eq:76} are $x^n = o(x)$. Then applying $x^n + x^m = o(x)$ to the Taylor series gives the result.
  \end{solution}
\end{exercise}

To help you recall the concept of \recall{conditional probability} consider the following question.
\begin{exercise}[\faPhoto] \label{ex:18}
  We have  one gift to give to one of three children. As we cannot
  divide the gift into parts, we decide to let `fate decide'. That
  is, we choose a random number in the set $\{1, 2, 3\}$. The first
  child that guesses the number wins the gift. Show that the
  probability of winning the gift is the same for each child.
  \begin{hint}
    For the second child, condition on the event that the first does not choose the right number.
    Use the definition of conditional probability:
    $\P{A|B} = \P{AB}/\P{B}$ provided $\P{B}>0$.
  \end{hint}
\begin{solution}
    The probability that the first child to guess also wins is
    $1/3$. What is the probability for child number two? Well, for
    him/her to win, it is necessary that child one does not win and
    that child two guesses the right number of the remaining
    numbers. Assume, without loss of generality that child 1 chooses
    $3$ and that this is not the right number. Then 
    \begin{equation*}
      \begin{split}
&\P{\text{Child  2 wins}} \\
&= \P{\text{Child 2 guesses the right number and child 1 does not win}} \\
&= \P{\text{Child 2 guesses the right number} \given \text{ child 1 does not win}}
\cdot \P{\text{Child 1 does not win}} \\
&= \P{\text{Child 2 makes the right guess in the set $\{1,2\}$}}\cdot \frac 23 \\
&= \frac 1 2\cdot \frac 23  = \frac 1 3.
      \end{split}
    \end{equation*}
    Similar conditional reasoning gives that child 3 wins with probability $1/3$. 
  \end{solution}
\end{exercise}

You should know that:
\begin{subequations}
\begin{align}
\P{A\given B} & = \frac{\P{AB}}{\P{B}}, \quad\text{ if } \P{B}>0, \\
  \P{A} &= \sum_{i=1}^n \P{A B_i} = \sum_{i=1}^n \P{A\given B_i} \P{B_i}, \quad\text{ if  $A=\bigcup_{i=1}^n B_i$ and $\P{B_i>0}$ for all $i$}. \label{eq:70}
\end{align}
\end{subequations}

You should know that for a non-negative, integer-valued random variable $X$ with  \recall{probability mass function} $f(k)= \P{X = k} = f(k)$, 
\begin{subequations}
\begin{align}
X&=\sum_{n=0}^\infty X\1{X=n} = \sum_{n=0}^\infty n \1{X=n},   \label{eq:77} \\
\E X &= \sum_{n=0}^\infty n f(n), \\
\E{g(X)} &= \sum_{n=0}^\infty g(n) f(n)  \label{eq:66}\\
\E{\1{X\leq x}} &= \P{X\leq x}, \label{eq:74}\\
\V X &= \E{X^2} - (\E X)^2,\label{eq:68}
\end{align}
\end{subequations}

\begin{exercise}[\faFlask]
  Why is~\eqref{eq:77} true?
  \begin{solution}
To see~\eqref{eq:77}, note first that $X\1{X=n} = n\1{X=n}$ because $X=n$ when $\1{X=n} =1$, and second that $\sum_{n=0}^\infty \1{X=n} =1$, since $X$ takes one of the values in $\N$, and events $\{X=n\}$ and $\{X=m\}$ are non-overlapping when $n\neq m$. 
  \end{solution}
\end{exercise}

\begin{exercise}[\faFlask]
Define  \recall{survivor function} of $X$ as $G(k) = \P{X>k}$. Show that
\begin{equation*}
  G(k) = \sum_{m=0}^\infty \1{m>k} f(m).
\end{equation*}
As you will see, this idea  makes the computation of certain expressions quite a bit easier. 
\end{exercise}

\begin{exercise}[\faCalculator]\label{ex:5}
  Express the probability mass  $f(k)$ and the survivor function $G(k)$ in terms of the \recall{distribution function} $F(k)=\P{X\leq k}$ of $X$.
  \begin{hint}
This exercise is just meant to become familiar with the notation.
  \end{hint}
  \begin{solution}
    \begin{align*}
    f(k) &= \P{X=k} = \P{X\leq k} - \P{X\leq k-1} = F(k)-F(k-1), \\
    G(k) &= \P{X>k} = 1 - \P{X\leq k} = 1-F(k).        
    \end{align*}
  \end{solution}
\end{exercise}

\begin{exercise}[\faFlask]
  Which of the following is true: $G(k) = 1-F(k)$, $G(k) = 1-F(k-1)$, or $G(k) = 1-F(k+1)$?
  \begin{solution}
  $G(k) = 1- F(k) = 1-\P{X\leq k} = \P{X>k}$. 
    It is all too easy to make, so called, off-by-one errors, such as
    in the three alternatives above.  I nearly always check simple
    cases to prevent such simple mistakes. I advise you to acquire the
    same habit.
  \end{solution}
\end{exercise}


\begin{exercise}[\faCalculator]\label{ex:6}
 Use indicator functions to prove that $\E X =  \sum_{k=0}^\infty G(k)$.
    \begin{hint}
Write 
$\sum_{k=0}^\infty G(k) = \sum_{k=0}^\infty \sum_{m=k+1}^\infty \P{X=m}$, reverse the summations. Then realize that $\sum_{k=0}^\infty \1{k<m} = m$. 
You should be aware that this sort of problem is just a regular probability
  theory problem, nothing fancy. We use/adapt the tools you learned in
  calculus to carry out 2D integrals (or in this case 2D summations).
    \end{hint}
\begin{solution}
Observe first that $\sum_{k=0}^\infty \1{m>k} = m$, since $\1{m>k}=1$ if $k<m$ and $\1{m>k} = 0$ if $k\geq m$. With this, 
\begin{align*}
\sum_{k=0}^\infty G(k) 
&= \sum_{k=0}^\infty \P{X>k} 
= \sum_{k=0}^\infty \sum_{m=k+1}^\infty \P{X=m}  \\
& = \sum_{k=0}^\infty \sum_{m=0}^\infty \1{m>k} \P{X=m} 
= \sum_{m=0}^\infty \sum_{k=0}^\infty \1{m>k} \P{X=m} \\
&= \sum_{m=0}^\infty m\P{X=m} = \E X.
\end{align*}
In case you are interested in mathematical justifications: the interchange of the two summations is allowed by Tonelli's theorem because the summands are all positive.
(Interchanging the order of summations or integration is not always allowed because the results can be different when part of the integrand is negative.
Check Fubini's theorem for more on this if you are interested.)
\end{solution}
\end{exercise}

\begin{exercise}[\faCalculator]\label{ex:66}
 Use indicator functions to prove that
$\sum_{i=0}^\infty i G(i) =  \E{X^2}/2 - \E{X}/2.$
    \begin{hint}
$\sum_{i=0}^\infty i G(i) = \sum_{n=0}^\infty \P{X=n} \sum_{i=0}^\infty i \1{n\geq i+1}$,
and reverse the summations.
    \end{hint}
\begin{solution}
\begin{align*}
\sum_{i=0}^\infty i G(i)
&= \sum_{i=0}^\infty i \sum_{n=i+1}^\infty \P{X=n} = \sum_{n=0}^\infty \P{X=n} \sum_{i=0}^\infty i \1{n\geq i+1} \\
&= \sum_{n=0}^\infty \P{X=n} \sum_{i=0}^{n-1}i  = \sum_{n=0}^\infty \P{X=n} \frac{(n-1)n}{2} \\
&= \sum_{n=0}^\infty  \frac{n^2}{2} \P{X=n} - \frac{\E X}{2}
= \frac{\E{X^2}}{2} - \frac{\E X}{2}.
\end{align*}
\end{solution}
\end{exercise}



Let $X$ be a continuous non-negative random variable with distribution function $F$.  We write 
\begin{equation*}
  \E{X} = \int_0^\infty x \d F(x)
\end{equation*}
for the expectation of $X$. Here $\d F(x)$ acts as a shorthand for $f(x) \d x$\footnote{For the interested reader, $\int x \d F(x)$ is a Lebesgue-Stieltjes integral with respect to the distribution function $F$.}. Recall that
\begin{align*}
\E{g(X)} &= \int_0^\infty g(x) \d F(x).
\end{align*}



\begin{exercise}[\faCalculator]
 Use indicator functions to prove that 
$   \E X = \int_0^\infty x \d F(x)  = \int_0^\infty G(y) \d y,$
where $G(x) = 1 - F(x)$. 
\begin{hint}
$\E X = \int_0^\infty x \d F(x)  = \int_0^\infty \int_0^\infty 1_{y\leq x} \d y \d F(x)$.
\end{hint}
\begin{solution}
\begin{equation*}
  \begin{split}
    \E{X} &= \int_0^\infty x \d F(x)  = \int_0^\infty \int_0^x \d y \d F(x) \\
    & = \int_0^\infty \int_0^\infty \1{y\leq x} \d y \d F(x)   = \int_0^\infty \int_0^\infty \1{y\leq x} \d F(x) \d y\\
    & = \int_0^\infty \int_y^\infty \d F(x) \d y = \int_0^\infty G(y) \d y.
  \end{split}
\end{equation*}
\end{solution}
\end{exercise}

You should be able to use indicator functions and integration by parts to show that $\E{X^2} = 2\int_0^\infty y G(y) \d y$, where $G(x) = 1- F(x)$, provide the second moment exists.

\begin{exercise}[\faCalculator]
 Use \emph{indicator functions} to prove that for a continuous non-negative random
    variable $X$ with distribution function $F$, 
$    \E{X^2} = \int_0^\infty x^2 \d F(x)  = 2 \int_0^\infty y G(y) \d y,$
where $G(x) = 1 - F(x)$. 
\begin{hint}
$\int_0^\infty y G(y) \d y = \int_0^\infty y \int_0^\infty \1{y\leq x}f(x)\, \d x \d y$.
\end{hint}
\begin{solution}
  \begin{align*}
\int_0^\infty y G(y) \d y 
&=  \int_0^\infty y \int_y^\infty f(x)\, \d x \d y =  \int_0^\infty y \int_0^\infty \1{y\leq x}f(x)\, \d x \d y\\
&=  \int_0^\infty f(x) \int_0^\infty y \1{y \leq x}\, \d y \d x
=  \int_0^\infty f(x) \int_0^x y\, \d y \d x\\
&=  \int_0^\infty f(x) \frac{x^2}2 \d x =\frac{\E{X^2}}2.
  \end{align*}
\end{solution}
\end{exercise}

\begin{exercise}[\faCalculator]
 Now use \emph{integration by parts} to show that for a continuous non-negative random
    variable $X$ with distribution function $F$ and survivor function $G=1-F$, 
$\int_0^\infty y G(y) \d y = \E{X^2}/2,$
\begin{solution}
  \begin{equation}
      \int_0^\infty y G(y) \d y 
= \frac{y^2}2 G(y) \bigg|_0^\infty  - \int_0^\infty \frac{y^2}2 g(y)\d y = \int_0^\infty \frac{y^2}2 f(y)\d y = \frac{\E{X^2}}2,
  \end{equation}
  since $g(y) = G'(y) = - F'(y) = - f(y)$. Note that we used $\frac{y^2}2 G(y) \bigg|_0^\infty = 0 - 0 = 0$, which follows from our assumption that $\E{X^2}$ exists, implying that $\lim_{y \to \infty} y^2G(y) = 0$.
\end{solution}
\end{exercise}


You should know that for  the \recall{moment-generating function}  $M_X(s)$ of a random variable~$X$ and~$s$ a real number sufficiently small  that the expectation(s) below exists: 
\begin{subequations}
\begin{align}
  M_X(s) &= \E{e^{s X}}, \\
  M_X(s) &= \text{uniquely characterizes the distribution of $X$}, \label{eq:75}\\
  \E{X} &= M_{X}'(0) = \left.\frac{\d M_{X}(s)}{d s}\right|_{s=0},\label{eq:69}\\
\E{(X^2} &= M_{X}''(0), \label{eq:64}\\
M_{X+Y}(s) &= M_X(s)\cdot M_Y(s), \quad \text{  if $X$ and $Y$ are independent}, \label{eq:73}
\end{align}
\end{subequations}

\begin{exercise}[\faFlask]
  What is $M_X(0)?$
  \begin{solution}
    $M_X(0) = \E{e^{0 X}} = \E{e^0} = \E{1} = 1.$
  \end{solution}
\end{exercise}

\Closesolutionfile{hint}
\Closesolutionfile{ans}

\opt{solutionfiles}{
\subsection*{Hints}
\input{hint}
\subsection*{Solutions}
\input{ans}
}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../queueing_book"
%%% End:
